from dataclasses import dataclass
import traceback
from typing import Callable
import warnings
import numpy as np
import pandas as pd
from pathlib import Path


# Local imports:
## Core:
# from .datawriter import DataWriter
# from .neurons import NeuronType, Neurons, BinnedSpiketrain, Mua
# from .probe import ProbeGroup
# from .position import Position
# from .epoch import Epoch #, NamedTimerange
# from .signal import Signal
# from .laps import Laps
# from .flattened_spiketrains import FlattenedSpiketrains

# from .. import DataWriter, NeuronType, Neurons, BinnedSpiketrain, Mua, ProbeGroup, Position, Epoch, Signal, Laps, FlattenedSpiketrains

from neuropy.core import DataWriter, NeuronType, Neurons, BinnedSpiketrain, Mua, ProbeGroup, Position, Epoch, Signal, Laps, FlattenedSpiketrains
from neuropy.core.session.dataSession import DataSession

from neuropy.io import NeuroscopeIO, BinarysignalIO 
# from ...io import NeuroscopeIO, BinarysignalIO # from neuropy.io import NeuroscopeIO, BinarysignalIO


from neuropy.utils.load_exported import import_mat_file
from neuropy.utils.mixins.print_helpers import ProgressMessagePrinter, SimplePrintable, OrderedMeta


@dataclass
class SessionFileSpec:
    """ Speccifies a specification for a single file.
    Members:
        session_load_callback: Callable[[Path, DataSession], DataSession] a function that takes the path to load and the session to load it into and performs the operation
    
    Examples: 
        SessionFileSpec('{}.xml', session_name, 'The primary .xml configuration file'), SessionFileSpec('{}.neurons.npy', session_name, 'The numpy data file containing information about neural activity.')
        SessionFileSpec('{}.probegroup.npy', session_name, 'The numpy data file containing information about the spatial layout of recording probes')
        SessionFileSpec('{}.position.npy', session_name, 'The numpy data file containing the recorded animal positions (as generated by optitrack) over time.')
        SessionFileSpec('{}.paradigm.npy', session_name, 'The numpy data file containing the recording epochs. Each epoch is defined as a: (label:str, t_start: float (in seconds), t_end: float (in seconds))')
    """
    fileSpecString: str
    suggestedBaseName: str
    description: str
    session_load_callback: Callable[[Path, DataSession], DataSession]
    
    @property
    def filename(self):
        """The filename property."""
        return self.fileSpecString.format(self.suggestedBaseName)
    
    def resolved_path(self, parent_path, overrideBasename=None):
        """Gets the resolved Path given the parent_path """
        if overrideBasename is not None:
            self.suggestedBaseName = overrideBasename
        return parent_path.joinpath(self.filename)
   
 
 

class SessionFolderSpecError(Exception):
    """ An exception raised when a session folder spec requirement fails """
    def __init__(self, message, failed_spec_item):
        self.message = message
        self.failed_spec_item = failed_spec_item
    def __str__(self):
        return self.message

 
 
class RequiredFileError(SessionFolderSpecError):
    """ An exception raised when a required file is missing """
    def __init__(self, message, missing_file_spec):
        self.message = message
        self.failed_spec_item = missing_file_spec
    def __str__(self):
        return self.message
    
class RequiredValidationFailedError(SessionFolderSpecError):
    """ An exception raised when a required validation spec fails """
    def __init__(self, message, failed_validation):
        self.message = message
        self.failed_spec_item = failed_validation
    def __str__(self):
        return self.message
    
    
class SessionFolderSpec():
    """ Documents the required and optional files for a given session format """
    def __init__(self, required = [], optional = [], additional_validation_requirements=[]) -> None:
        # additiona_validation_requirements: a list of callbacks that are passed the proposed_session_path on self.validate(...) and return True/False. All must return true for validate to succeed.
        self.required_files = required
        self.optional_files = optional
        self.additional_validation_requirements = additional_validation_requirements
        
    def __repr__(self) -> str:
        return f"<{self.__class__.__name__}: {self.__dict__};>"


    def resolved_paths(self, proposed_session_path):
        """ Gets whether the proposed_session_path meets the requirements and returns the resolved paths if it can.
            Does not check whether any of the files exist, it just builds the paths
            
        Returns:
            two dictionaries containing the resolved path:file_spec pairs
        """
        proposed_session_path = Path(proposed_session_path)
        # build absolute paths from the proposed_session_path and the files
        # resolved_required_files = [a_file_spec.resolved_path(proposed_session_path) for a_file_spec in self.required_files]
        # resolved_optional_files = [a_file_spec.resolved_path(proposed_session_path) for a_file_spec in self.optional_files]
        resolved_required_filespecs_dict = {a_file_spec.resolved_path(proposed_session_path):a_file_spec for a_file_spec in self.required_files}
        resolved_optional_filespecs_dict = {a_file_spec.resolved_path(proposed_session_path):a_file_spec for a_file_spec in self.optional_files}
        return resolved_required_filespecs_dict, resolved_optional_filespecs_dict
        
    def validate(self, proposed_session_path):
        """Check whether the proposed_session_path meets this folder spec's requirements
        Args:
            proposed_session_path ([Path]): [description]

        Returns:
            [Bool]: [description]
        """
        resolved_required_filespecs_dict, resolved_optional_filespecs_dict = self.resolved_paths(proposed_session_path=proposed_session_path)
            
        meets_spec = False
        if not Path(proposed_session_path).exists():
            meets_spec = False # the path doesn't even exist, it can't be valid
        else:
            # the path exists:
            for a_required_filepath, a_file_spec in resolved_required_filespecs_dict.items():
                if not a_required_filepath.exists():
                    meets_spec = False                    
                    raise RequiredFileError(f'Required File: {a_required_filepath} does not exist.', (a_required_filepath, a_file_spec))
                    break
            for a_required_validation_function in self.additional_validation_requirements:
                if not a_required_validation_function(Path(proposed_session_path)):
                    # print('Required additional_validation_requirements[i]({}) returned False'.format(proposed_session_path))
                    meets_spec = False
                    raise RequiredValidationFailedError(f'Required additional_validation_requirements[i]({proposed_session_path}) returned False', a_required_validation_function)
                    break
                
            for an_optional_filepath, a_file_spec in resolved_optional_filespecs_dict.items():
                if not an_optional_filepath.exists():
                    # print('WARNING: Optional File: {} does not exist.'.format(an_optional_file))
                    warnings.warn(f'WARNING: Optional File: "{an_optional_filepath}" does not exist. Continuing without it.')
                    
            meets_spec = True # otherwise it exists
            
        return meets_spec, resolved_required_filespecs_dict, resolved_optional_filespecs_dict
    

# session_name = '2006-6-07_11-26-53'
# SessionFolderSpec(required=['{}.xml'.format(session_name),
#                             '{}.spikeII.mat'.format(session_name), 
#                             '{}.position_info.mat'.format(session_name),
#                             '{}.epochs_info.mat'.format(session_name), 
# ])


class SessionConfig(SimplePrintable, metaclass=OrderedMeta):
    """A simple data structure that holds the information specifying a data session, such as the basepath, session_spec, and session_name
    """
    
    @property
    def resolved_required_file_specs(self):
        """The resolved_required_file_specs property."""
        return {a_filepath:(lambda sess, filepath=a_filepath: a_spec.session_load_callback(filepath, sess)) for a_filepath, a_spec in self.resolved_required_filespecs_dict.items()}
        
    @property
    def resolved_optional_file_specs(self):
        """The resolved_required_file_specs property."""
        return {a_filepath:(lambda sess, filepath=a_filepath: a_spec.session_load_callback(filepath, sess)) for a_filepath, a_spec in self.resolved_optional_filespecs_dict.items()}
    
    
    def __init__(self, basepath, session_spec, session_name):
        """[summary]
        Args:
            basepath (pathlib.Path): [description].
            session_spec (SessionFolderSpec): used to load the files
            session_name (str, optional): [description].
        """
        self.basepath = basepath
        self.session_name = session_name
        # Session spec:
        self.session_spec=session_spec
        self.is_resolved, self.resolved_required_filespecs_dict, self.resolved_optional_filespecs_dict = self.session_spec.validate(self.basepath)
        
    def validate(self):
        """ re-validates the self.session_spec items and updates the resolved dicts """
        self.is_resolved, self.resolved_required_filespecs_dict, self.resolved_optional_filespecs_dict = self.session_spec.validate(self.basepath)

    

class DataSessionLoader:
    """ An extensible class that performs session data loading operations. 
        Data might be loaded into a Session object from many different source formats depending on lab, experimenter, and age of the data.
        Often this data needs to be reverse engineered and translated into the correct format, which is a tedious and time-consuming process.
        This class allows clearly defining and documenting the requirements of a given format once it's been reverse-engineered.
        
        Primary usage methods:
            DataSessionLoader.bapun_data_session(basedir)
            DataSessionLoader.kdiba_old_format_session(basedir)
    """
    # def __init__(self, load_function, load_arguments=dict()):        
    #     self.load_function = load_function
    #     self.load_arguments = load_arguments
        
    # def load(self, updated_load_arguments=None):
    #     if updated_load_arguments is not None:
    #         self.load_arguments = updated_load_arguments
                 
    #     return self.load_function(self.load_arguments)
    
    pix2cm = 287.7698 # constant conversion factor for spikeII and IIdata (KDiba) formats
    
    #######################################################
    ## Public Methods:
    #######################################################
    
    # KDiba Old Format:
    @staticmethod
    def bapun_data_session(basedir):
        def bapun_data_get_session_name(basedir):
            # Find the only .xml file to obtain the session name
            xml_files = sorted(basedir.glob("*.xml"))        
            assert len(xml_files) == 1, f"Found more than one .xml file. Found files: {xml_files}"
            file_prefix = xml_files[0].with_suffix("") # gets the session name (basically) without the .xml extension. (R:\data\Bapun\Day5TwoNovel\RatS-Day5TwoNovel-2020-12-04_07-55-09)   
            file_basename = xml_files[0].stem # file_basename: (RatS-Day5TwoNovel-2020-12-04_07-55-09)
            # print('file_prefix: {}\nfile_basename: {}'.format(file_prefix, file_basename))
            return file_basename # 'RatS-Day5TwoNovel-2020-12-04_07-55-09'
        def get_session_obj(config):
            curr_args_dict = dict()
            curr_args_dict['basepath'] = config.basepath
            curr_args_dict['session_obj'] = DataSession(config)
            return DataSessionLoader._default_load_bapun_npy_session_folder(curr_args_dict)
            
        session_name = bapun_data_get_session_name(basedir) # 'RatS-Day5TwoNovel-2020-12-04_07-55-09'
        session_spec = SessionFolderSpec(required=[fname.format(session_name) for fname in ['{}.xml','{}.neurons.npy','{}.probegroup.npy','{}.position.npy','{}.paradigm.npy']])
        session_config = SessionConfig(basedir, session_spec=session_spec, session_name=session_name)
        assert session_config.is_resolved, "active_sess_config could not be resolved!"
        return get_session_obj(session_config)
        
    # KDiba Old Format:
    def kdiba_old_format_session(basedir):
        def kdiba_old_format_get_session_name(basedir):
            return Path(basedir).parts[-1]
        def get_session_obj(config):
            curr_args_dict = dict()
            curr_args_dict['basepath'] = config.basepath
            curr_args_dict['session_obj'] = DataSession(config)
            return DataSessionLoader._default_kdiba_flat_spikes_load_session_folder(curr_args_dict)
        session_name = kdiba_old_format_get_session_name(basedir) # session_name = '2006-6-07_11-26-53'
        session_spec = SessionFolderSpec(required=[fname.format(session_name) for fname in ['{}.xml','{}.spikeII.mat','{}.position_info.mat','{}.epochs_info.mat']])
        session_config = SessionConfig(basedir, session_spec=session_spec, session_name=session_name)
        assert session_config.is_resolved, "active_sess_config could not be resolved!"
        return get_session_obj(session_config)
    
    
    #######################################################
    ## Internal Methods:
    #######################################################
    @staticmethod
    def _default_extended_postload(fp, session):
        # Computes Common Extended properties:
        ## Ripples:
        active_file_suffix = '.ripple.npy'
        found_datafile = Epoch.from_file(fp.with_suffix(active_file_suffix))
        if found_datafile is not None:
            print('Loading success: {}.'.format(active_file_suffix))
            session.ripple = found_datafile
        else:
            # Otherwise load failed, perform the fallback computation
            print('Failure loading {}. Must recompute.\n'.format(active_file_suffix))
            session.ripple = DataSession.compute_neurons_ripples(session, save_on_compute=True)

        ## MUA:
        active_file_suffix = '.mua.npy'
        found_datafile = Mua.from_file(fp.with_suffix(active_file_suffix))
        if found_datafile is not None:
            print('Loading success: {}.'.format(active_file_suffix))
            session.mua = found_datafile
        else:
            # Otherwise load failed, perform the fallback computation
            print('Failure loading {}. Must recompute.\n'.format(active_file_suffix))
            session.mua = DataSession.compute_neurons_mua(session, save_on_compute=True)

        ## PBE Epochs:
        active_file_suffix = '.pbe.npy'
        found_datafile = Epoch.from_file(fp.with_suffix(active_file_suffix))
        if found_datafile is not None:
            print('Loading success: {}.'.format(active_file_suffix))
            session.pbe = found_datafile
        else:
            # Otherwise load failed, perform the fallback computation
            print('Failure loading {}. Must recompute.\n'.format(active_file_suffix))
            session.pbe = DataSession.compute_pbe_epochs(session, save_on_compute=True)
            
        
        # add PBE information to spikes_df from session.pbe
        DataSessionLoader._default_add_spike_PBEs_if_needed(session)
        DataSessionLoader._default_add_spike_scISIs_if_needed(session)
        # return the session with the upadated member variables
        return session
    
    @staticmethod
    def _default_compute_spike_interpolated_positions_if_needed(session, spikes_df, time_variable_name='t_rel_seconds', force_recompute=True):     
        ## Positions:
        active_file_suffix = '.interpolated_spike_positions.npy'
        if not force_recompute:
            found_datafile = FlattenedSpiketrains.from_file(session.filePrefix.with_suffix(active_file_suffix))
        else:
            found_datafile = None
        if found_datafile is not None:
            print('\t Loading success: {}.'.format(active_file_suffix))
            session.flattened_spiketrains = found_datafile
        else:
            # Otherwise load failed, perform the fallback computation
            print('\t Failure loading {}. Must recompute.\n'.format(active_file_suffix))
            spikes_df = FlattenedSpiketrains.interpolate_spike_positions(spikes_df, session.position.time, session.position.x, session.position.y, position_linear_pos=session.position.linear_pos, position_speeds=session.position.speed, spike_timestamp_column_name=time_variable_name)
            session.flattened_spiketrains = FlattenedSpiketrains(spikes_df, time_variable_name=time_variable_name, t_start=0.0)
            
            session.flattened_spiketrains.filename = session.filePrefix.with_suffix(active_file_suffix)
            # print('\t Saving updated interpolated spike position results to {}...'.format(session.flattened_spiketrains.filename), end='')
            with ProgressMessagePrinter(session.flattened_spiketrains.filename, '\t Saving', 'updated interpolated spike position results'):
                session.flattened_spiketrains.save()
            # print('\t done.\n')
    
        # return the session with the upadated member variables
        return session, spikes_df
    
    
    
    @staticmethod
    def _default_compute_linear_position_if_needed(session, force_recompute=True):
        # TODO: this is not general, this is only used for this particular flat kind of file:
            # Load or compute linear positions if needed:
        if (not session.position.has_linear_pos):
            ## compute linear positions: 
            ## Positions:
            active_file_suffix = '.position.npy'
            if not force_recompute:
                found_datafile = Position.from_file(session.filePrefix.with_suffix(active_file_suffix))
            else:
                found_datafile = None
            if found_datafile is not None:
                print('Loading success: {}.'.format(active_file_suffix))
                session.position = found_datafile
            else:
                # Otherwise load failed, perform the fallback computation
                print('Failure loading {}. Must recompute.\n'.format(active_file_suffix))
                session.position = DataSession.compute_linear_position(session)
            
            session.position.filename = session.filePrefix.with_suffix(active_file_suffix)
            # print('Saving updated position results to {}...'.format(session.position.filename), end='')
            with ProgressMessagePrinter(session.position.filename, 'Saving', 'updated position results'):
                session.position.save()
            # print('\t done.\n')
        else:
            print('\t linearized position loaded from file.')
            # return the session with the upadated member variables
        return session

    @staticmethod
    def _default_add_spike_PBEs_if_needed(session):
        updated_spk_df = session.compute_spikes_PBEs()
        return session
    
    @staticmethod
    def _default_add_spike_scISIs_if_needed(session):
        with ProgressMessagePrinter('filepath?', 'Computing', 'added spike scISI column'):
            updated_spk_df = session.spikes_df.spikes.add_same_cell_ISI_column()
        return session
    
    
    
        
    #######################################################
    ## Bapun Nupy Format Only Methods:
    @staticmethod
    def __default_compute_bapun_flattened_spikes(session, timestamp_scale_factor=(1/1E4)):
        spikes_df = FlattenedSpiketrains.build_spike_dataframe(session)
        session.flattened_spiketrains = FlattenedSpiketrains(spikes_df, t_start=session.neurons.t_start) # FlattenedSpiketrains(spikes_df)
        print('\t Done!')
        return session
    
    
    @staticmethod
    def _default_load_bapun_npy_session_folder(args_dict):        
        basepath = args_dict['basepath']
        session = args_dict['session_obj']
        
        basepath = Path(basepath)
        xml_files = sorted(basepath.glob("*.xml"))
        assert len(xml_files) > 0, "Missing required .xml file!"
        assert len(xml_files) == 1, f"Found more than one .xml file. Found files: {xml_files}"

        fp = xml_files[0].with_suffix("") # gets the session name (basically) without the .xml extension.
        session.filePrefix = fp
        session.recinfo = NeuroscopeIO(xml_files[0])

        # if session.recinfo.eeg_filename.is_file():
        try:
            session.eegfile = BinarysignalIO(
                session.recinfo.eeg_filename,
                n_channels=session.recinfo.n_channels,
                sampling_rate=session.recinfo.eeg_sampling_rate,
            )
        except ValueError:
            print('session.recinfo.eeg_filename exists ({}) but file cannot be loaded in the appropriate format. Skipping. \n'.format(session.recinfo.eeg_filename))
            session.eegfile = None

        if session.recinfo.dat_filename.is_file():
            session.datfile = BinarysignalIO(
                session.recinfo.dat_filename,
                n_channels=session.recinfo.n_channels,
                sampling_rate=session.recinfo.dat_sampling_rate,
            )
        else:
            session.datfile = None

        session_name = session.name
        print('\t basepath: {}\n\t session_name: {}'.format(basepath, session_name)) # session_name: 2006-6-08_14-26-15
        
        session.neurons = Neurons.from_file(fp.with_suffix(".neurons.npy")) # Loads the Neurons from file if possible
        session.probegroup = ProbeGroup.from_file(fp.with_suffix(".probegroup.npy"))
        session.position = Position.from_file(fp.with_suffix(".position.npy"))
        
        # ['.neurons.npy','.probegroup.npy','.position.npy','.paradigm.npy']
        #  [fname.format(session_name) for fname in ['{}.xml','{}.neurons.npy','{}.probegroup.npy','{}.position.npy','{}.paradigm.npy']]
        session.paradigm = Epoch.from_file(fp.with_suffix(".paradigm.npy"))  # "epoch" field of file



        ###############################################################
        ## END LOAD PORTION:



        # session = DataSessionLoader.__default_compute_bapun_flattened_spikes(session)
        
        # Load or compute linear positions if needed:        
        if (not session.position.has_linear_pos):
            # compute linear positions:
            print('computing linear positions for all active epochs for session...')
            # end result will be session.computed_traces of the same length as session.traces in terms of frames, with all non-maze times holding NaN values
            session.position.linear_pos = np.full_like(session.position.time, np.nan)
            acitve_epoch_timeslice_indicies1, active_positions_maze1, linearized_positions_maze1 = DataSession.compute_linearized_position(session, 'maze1')
            acitve_epoch_timeslice_indicies2, active_positions_maze2, linearized_positions_maze2 = DataSession.compute_linearized_position(session, 'maze2')
            session.position.linear_pos[acitve_epoch_timeslice_indicies1] = linearized_positions_maze1.traces
            session.position.linear_pos[acitve_epoch_timeslice_indicies2] = linearized_positions_maze2.traces
            session.position.filename = session.filePrefix.with_suffix(".position.npy")
            # print('Saving updated position results to {}...'.format(session.position.filename))
            with ProgressMessagePrinter(session.position.filename, 'Saving', 'updated position results'):
                session.position.save()
            # print('done.\n')
        else:
            print('linearized position loaded from file.')

        ## Load or compute flattened spikes since this format of data has the spikes ordered only by cell_id:
        ## flattened.spikes:
        active_file_suffix = '.flattened.spikes.npy'
        found_datafile = FlattenedSpiketrains.from_file(fp.with_suffix(active_file_suffix))
        if found_datafile is not None:
            print('Loading success: {}.'.format(active_file_suffix))
            session.flattened_spiketrains = found_datafile
        else:
            # Otherwise load failed, perform the fallback computation
            print('Failure loading {}. Must recompute.\n'.format(active_file_suffix))
            session = DataSessionLoader.__default_compute_bapun_flattened_spikes(session) # sets session.flattened_spiketrains
            session.flattened_spiketrains.filename = session.filePrefix.with_suffix(active_file_suffix) # '.flattened.spikes.npy'
            print('\t Saving computed flattened spiketrains results to {}...'.format(session.flattened_spiketrains.filename), end='')
            session.flattened_spiketrains.save()
            print('\t done.\n')
        
        # Common Extended properties:
        session = DataSessionLoader._default_extended_postload(fp, session)

        return session # returns the session when done





    #######################################################
    ## KDiba Old Format Only Methods:
    ## relies on _load_kamran_spikeII_mat, _default_spikeII_compute_laps_vars, __default_spikeII_compute_neurons, __default_load_kamran_exported_mats, _default_compute_linear_position_if_needed
    @staticmethod
    def _default_kdiba_flat_spikes_load_session_folder(args_dict):
        ## relies on _load_kamran_spikeII_mat, _default_spikeII_compute_laps_vars, __default_spikeII_compute_neurons, default_load_kamran_IIdata_mat, _default_compute_linear_position_if_needed
        basepath = args_dict['basepath']
        session = args_dict['session_obj']
        # timestamp_scale_factor = (1/1E6)
        # timestamp_scale_factor = (1/1E4)
        timestamp_scale_factor = 1.0
                     
        # active_time_variable_name = 't' # default
        # active_time_variable_name = 't_seconds' # use converted times (into seconds)
        active_time_variable_name = 't_rel_seconds' # use converted times (into seconds)

        basepath = Path(basepath)
        xml_files = sorted(basepath.glob("*.xml"))
        assert len(xml_files) > 0, "Missing required .xml file!"
        assert len(xml_files) == 1, f"Found more than one .xml file. Found files: {xml_files}"

        fp = xml_files[0].with_suffix("")
        session.filePrefix = fp
        session.recinfo = NeuroscopeIO(xml_files[0])

        try:
            session.eegfile = BinarysignalIO(
                session.recinfo.eeg_filename,
                n_channels=session.recinfo.n_channels,
                sampling_rate=session.recinfo.eeg_sampling_rate,
            )
        except ValueError:
            print('session.recinfo.eeg_filename exists ({}) but file cannot be loaded in the appropriate format. Skipping. \n'.format(session.recinfo.eeg_filename))
            session.eegfile = None

        if session.recinfo.dat_filename.is_file():
            session.datfile = BinarysignalIO(
                session.recinfo.dat_filename,
                n_channels=session.recinfo.n_channels,
                sampling_rate=session.recinfo.dat_sampling_rate,
            )
        else:
            session.datfile = None
            
        session_name = session.name
        print('\t basepath: {}\n\t session_name: {}'.format(basepath, session_name)) # session_name: 2006-6-08_14-26-15


        ###############################################################
        ## END LOAD PORTION:
        
        
        # IIdata.mat file Position and Epoch:
        session = DataSessionLoader.__default_kdiba_exported_load_mats(basepath, session_name, session, time_variable_name=active_time_variable_name)
        
        ## .spikeII.mat file:
        try:
            spikes_df, flat_spikes_out_dict = DataSessionLoader.__default_kdiba_pho_exported_spikeII_load_mat(session, timestamp_scale_factor=timestamp_scale_factor)
        except FileNotFoundError as e:
            print('FileNotFoundError: {}.\n Trying to fall back to original .spikeII.mat file...'.format(e))
            spikes_df, flat_spikes_out_dict = DataSessionLoader.__default_kdiba_spikeII_load_mat(session, timestamp_scale_factor=timestamp_scale_factor)
            
        except Exception as e:
            # print('e: {}.\n Trying to fall back to original .spikeII.mat file...'.format(e))
            track = traceback.format_exc()
            print(track)
            raise e
        else:
            pass
        
        # Load or compute linear positions if needed:
        session = DataSessionLoader._default_compute_linear_position_if_needed(session)
        
        ## Testing: Fixing spike positions
        spikes_df['x_loaded'] = spikes_df['x']
        spikes_df['y_loaded'] = spikes_df['y']
        session, spikes_df = DataSessionLoader._default_compute_spike_interpolated_positions_if_needed(session, spikes_df, time_variable_name=active_time_variable_name)
        # spikes_df = FlattenedSpiketrains.interpolate_spike_positions(spikes_df, session.position.time, session.position.x, session.position.y, position_linear_pos=session.position.linear_pos, position_speeds=session.position.speed, spike_timestamp_column_name=active_time_variable_name)
        
        ## Laps:
        try:
            session, laps_df = DataSessionLoader.__default_kdiba_spikeII_load_laps_vars(session, time_variable_name=active_time_variable_name)
        except Exception as e:
            # raise e
            print('session.laps could not be loaded from .spikes.mat due to error {}. Computing.'.format(e))
            session, spikes_df = DataSessionLoader.__default_kdiba_spikeII_compute_laps_vars(session, spikes_df, active_time_variable_name)
        else:
            # Successful!
            print('session.laps loaded successfully!')
            pass

        ## Neurons (by Cell):
        session = DataSessionLoader.__default_kdiba_spikeII_compute_neurons(session, spikes_df, flat_spikes_out_dict, active_time_variable_name)
        session.probegroup = ProbeGroup.from_file(fp.with_suffix(".probegroup.npy"))
        
        # add the linear_pos to the spikes_df before building the FlattenedSpiketrains object:
        # spikes_df['lin_pos'] = session.position.linear_pos

        # add the flat spikes to the session so they don't have to be recomputed:
        session.flattened_spiketrains = FlattenedSpiketrains(spikes_df, time_variable_name=active_time_variable_name)
        
        # Common Extended properties:
        session = DataSessionLoader._default_extended_postload(fp, session)
        session.is_loaded = True # indicate the session is loaded
        return session # returns the session when done



        