import numpy as np
import pandas as pd
from pathlib import Path
from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatBaseRegisteredClass
from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import BapunDataSessionFormatRegisteredClass
from neuropy.core.session.dataSession import DataSession
from neuropy.core.session.Formats.SessionSpecifications import SessionFolderSpec, SessionFileSpec

# For specific load functions:
from neuropy.core import DataWriter, NeuronType, Neurons, BinnedSpiketrain, Mua, ProbeGroup, Position, Epoch, Signal, Laps, FlattenedSpiketrains
from neuropy.utils.mixins.print_helpers import ProgressMessagePrinter, SimplePrintable, OrderedMeta


class RachelDataSessionFormat(BapunDataSessionFormatRegisteredClass):
    """

    # Example Filesystem Hierarchy:
	ðŸ“¦Rachel
	â”£ ðŸ“‚merged_M1_20211123_raw_phy
	â”ƒ â”£ ðŸ“œwhitening_mat_inv.npy
	â”ƒ â”£ ðŸ“œspike_templates.npy
	â”ƒ â”£ ðŸ“œtempClustering.klg.3
	â”ƒ â”£ ðŸ“œcluster_info.tsv
	â”ƒ â”£ ðŸ“œchannel_shanks.npy
	â”ƒ â”£ ðŸ“œmerged_M1_20211123_raw.eeg
	â”ƒ â”£ ðŸ“œcluster_purity.tsv
	â”ƒ â”£ ðŸ“œspike_clusters.npy
	â”ƒ â”£ ðŸ“œsimilar_templates.npy
	â”ƒ â”£ ðŸ“œpc_feature_ind.npy
	â”ƒ â”£ ðŸ“œphy.log
	â”ƒ â”£ ðŸ“œmerged_M1_20211123_raw.paradigm.npy
	â”ƒ â”£ ðŸ“œcluster_group.tsv
	â”ƒ â”£ ðŸ“œspike_times.npy
	â”ƒ â”£ ðŸ“œtempClustering.fet.3
	â”ƒ â”£ ðŸ“œtempClustering.clu.3
	â”ƒ â”£ ðŸ“œmerged_M1_20211123_raw.xml
	â”ƒ â”£ ðŸ“œwhitening_mat.npy
	â”ƒ â”£ ðŸ“œtemplates.npy
	â”ƒ â”£ ðŸ“œparams.py
	â”ƒ â”£ ðŸ“œchannel_map.npy
	â”ƒ â”£ ðŸ“œpc_features.npy
	â”ƒ â”£ ðŸ“œchannel_positions.npy
	â”ƒ â”£ ðŸ“œttl_check.ipynb
	â”ƒ â”£ ðŸ“œamplitudes.npy
	â”ƒ â”£ ðŸ“œmerged_M1_20211123_raw.position.npy
	â”ƒ â”£ ðŸ“œtempClustering.temp.clu.3
	â”ƒ â”£ ðŸ“œmerged_M1_20211123_raw.neurons.npy
	â”ƒ â”£ ðŸ“œmerged_M1_20211123_raw.probegroup.npy
	â”ƒ â”— ðŸ“œcluster_q.tsv
    
    
    By default it attempts to find the single *.xml file in the root of this basedir, from which it determines the `session_name` as the stem (the part before the extension) of this file:
        basedir: Path(r'R:\data\Rachel\merged_M1_20211123_raw_phy')
        session_name: 'merged_M1_20211123_raw'
    
    From here, a list of known files to load from is determined:
        
    Usage:
        from neuropy.core.session.Formats.BaseDataSessionFormats import DataSessionFormatRegistryHolder, DataSessionFormatBaseRegisteredClass
        from neuropy.core.session.Formats.Specific.BapunDataSessionFormat import RachelDataSessionFormat

        _test_session = RachelDataSessionFormat.build_session(Path(r'R:\data\Rachel\merged_M1_20211123_raw_phy'))
        _test_session, loaded_file_record_list = RachelDataSessionFormat.load_session(_test_session)
        _test_session
        
    """
    
    _time_variable_name = 't_seconds' # It's 't_rel_seconds' for kdiba-format data for example or 't_seconds' for Bapun-format data
   
    @classmethod
    def get_session_name(cls, basedir):
        """ returns the session_name for this basedir, which determines the files to load. """
        # Find the only .xml file to obtain the session name
        return DataSessionFormatBaseRegisteredClass.find_session_name_from_sole_xml_file(basedir) # 'merged_M1_20211123_raw'

    @classmethod
    def get_session_spec(cls, session_name):
        return SessionFolderSpec(required=[SessionFileSpec('{}.xml', session_name, 'The primary .xml configuration file', cls._load_xml_file),
                                           SessionFileSpec('{}.neurons.npy', session_name, 'The numpy data file containing information about neural activity.', cls._load_neurons_file),
                                           SessionFileSpec('{}.probegroup.npy', session_name, 'The numpy data file containing information about the spatial layout of recording probes', cls._load_probegroup_file),
                                           SessionFileSpec('{}.position.npy', session_name, 'The numpy data file containing the recorded animal positions (as generated by optitrack) over time.', cls._load_position_file),
                                           SessionFileSpec('{}.paradigm.npy', session_name, 'The numpy data file containing the recording epochs. Each epoch is defined as a: (label:str, t_start: float (in seconds), t_end: float (in seconds))', cls._load_paradigm_file)]
                    )
    ### Specific Load Functions used in the session_spec
    @classmethod
    def _load_neurons_file(cls, filepath, session): # .neurons
        session.neurons = Neurons.from_file(filepath)
        return session
    @classmethod
    def _load_probegroup_file(cls, filepath, session): # .probegroup
        session.probegroup = ProbeGroup.from_file(filepath)
        return session
    @classmethod
    def _load_position_file(cls, filepath, session): # .position
        session.position = Position.from_file(filepath)
        return session
    @classmethod
    def _load_paradigm_file(cls, filepath, session): # .paradigm
        session.paradigm = Epoch.from_file(filepath)  # "epoch" field of file
        return session
    
    #######################################################
    ## Rachel Nupy Format Only Methods:
    @classmethod
    def _rachel_add_missing_spikes_df_columns(cls, spikes_df, neurons_obj):
        spikes_df, neurons_obj._reverse_cellID_index_map = spikes_df.spikes.rebuild_fragile_linear_neuron_IDXs()
        spikes_df['t'] = spikes_df['t_seconds'] # add the 't' column required for visualization
    
    
    ## Main load function:
    @classmethod
    def load_session(cls, session, debug_print=False):
        session, loaded_file_record_list = DataSessionFormatBaseRegisteredClass.load_session(session, debug_print=debug_print) # call the super class load_session(...) to load the common things (.recinfo, .filePrefix, .eegfile, .datfile)
        remaining_required_filespecs = {k: v for k, v in session.config.resolved_required_filespecs_dict.items() if k not in loaded_file_record_list}
        if debug_print:
            print(f'remaining_required_filespecs: {remaining_required_filespecs}')
        
        for file_path, file_spec in remaining_required_filespecs.items():
            session = file_spec.session_load_callback(file_path, session)
            loaded_file_record_list.append(file_path)
        
        # ['.neurons.npy','.probegroup.npy','.position.npy','.paradigm.npy']
        # session = DataSessionLoader.__default_compute_bapun_flattened_spikes(session)
        
        # Load or compute linear positions if needed:        
        if (not session.position.has_linear_pos):
            # compute linear positions:
            print('computing linear positions for all active epochs for session...')
            # end result will be session.computed_traces of the same length as session.traces in terms of frames, with all non-maze times holding NaN values
            session.position.linear_pos = np.full_like(session.position.time, np.nan)
            acitve_epoch_timeslice_indicies1, active_positions_maze1, linearized_positions_maze1 = DataSession.compute_linearized_position(session, 'maze')
            session.position.linear_pos[acitve_epoch_timeslice_indicies1] = linearized_positions_maze1.traces
            session.position.filename = session.filePrefix.with_suffix(".position.npy")
            # print('Saving updated position results to {}...'.format(session.position.filename))
            with ProgressMessagePrinter(session.position.filename, 'Saving', 'updated position results'):
                session.position.save()
            # print('done.\n')
        else:
            print('linearized position loaded from file.')

        ## Load or compute flattened spikes since this format of data has the spikes ordered only by cell_id:
        ## flattened.spikes:
        # active_file_suffix = '.flattened.spikes.npy'
        active_file_suffix = '.flattened.spikes.npy'
        found_datafile = FlattenedSpiketrains.from_file(session.filePrefix.with_suffix(active_file_suffix))
        if found_datafile is not None:
            print('Loading success: {}.'.format(active_file_suffix))
            session.flattened_spiketrains = found_datafile
        else:
            # Otherwise load failed, perform the fallback computation
            print('Failure loading {}. Must recompute.\n'.format(active_file_suffix))
            session = cls._default_compute_bapun_flattened_spikes(session, spike_timestamp_column_name=cls._time_variable_name) # sets session.flattened_spiketrains
            
            ## Testing: Fixing spike positions
            session, session.spikes_df = cls._default_compute_spike_interpolated_positions_if_needed(session, session.spikes_df, time_variable_name=cls._time_variable_name)
            cls._rachel_add_missing_spikes_df_columns(session.spikes_df, session.neurons) # add the missing columns to the dataframe             
            
            
            session.flattened_spiketrains.filename = session.filePrefix.with_suffix(active_file_suffix) # '.flattened.spikes.npy'
            print('\t Saving computed flattened spiketrains results to {}...'.format(session.flattened_spiketrains.filename), end='')
            session.flattened_spiketrains.save()
            print('\t done.\n')
        
        # Common Extended properties:
        session = cls._default_extended_postload(session.filePrefix, session)
        
        return session, loaded_file_record_list
    
    
    