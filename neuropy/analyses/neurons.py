import os
from dataclasses import dataclass
from pathlib import Path

import h5py
import matplotlib as mpl
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.signal as sg
from scipy.ndimage import gaussian_filter
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler


from ..utils.ccg import correlograms
from ..parsePath import Recinfo
from ..plotUtil import pretty_plot
from ..core import Neurons


class Spikes:
    """Spike related methods"""

    colors = {"pyr": "#211c1c", "intneur": "#3a924d", "mua": "#b4b2b1"}

    def __init__(self, basepath):

        if isinstance(basepath, Recinfo):
            self._obj = basepath
        else:
            self._obj = Recinfo(basepath)

        @dataclass
        class files:
            spikes: str = Path(str(filePrefix) + "_spikes.npy")
            instfiring: str = Path(str(filePrefix) + "_instfiring.pkl")

        self.files = files()

        filename = self.files.spikes
        if filename.is_file():
            self.load_spikes(filename)

    def load_rough_mua(self, mua_filename=None):
        """Load in multi-unit activity (MUA) generated by running spyking-circus with the 'thresholding' flag.
        Note that this differs from MUA designated in phy after manual spike sorting (q = 6),
        so it is defined as Spikes.rough_mua"""

        # First grab the mua file if not specified.
        if mua_filename is None:
            mua_filename = list(self._obj.basePath.glob("**/*.mua.hdf5"))
            assert (
                len(mua_filename) == 1
            ), "More than one .mua.hdf5 file found in directory tree. Re-organize and try again!"

        # Now load it in
        self.rough_mua = h5py.File(mua_filename[0], "r+")

    @property
    def instfiring(self):
        if self.files.instfiring.is_file():
            return pd.read_pickle(self.files.instfiring)
        else:
            print("instantenous file does not exist ")

    def gen_instfiring(self):
        spkall = np.concatenate(self.times)

        bins = np.arange(spkall.min(), spkall.max(), 0.001)

        spkcnt = np.histogram(spkall, bins=bins)[0]
        gaussKernel = self._gaussian()
        instfiring = sg.convolve(spkcnt, gaussKernel, mode="same", method="direct")

        data = pd.DataFrame({"time": bins[1:], "frate": instfiring})
        data.to_pickle(self.files.instfiring)
        return data

    def _gaussian(self):
        """Gaussian function for generating instantenous firing rate

        Returns:
            [array] -- [gaussian kernel centered at zero and spans from -1 to 1 seconds]
        """

        sigma = 0.020
        binSize = 0.001
        t_gauss = np.arange(-1, 1, binSize)
        A = 1 / np.sqrt(2 * np.pi * sigma ** 2)
        gaussian = A * np.exp(-(t_gauss ** 2) / (2 * sigma ** 2))

        return gaussian

    def plot_ccg(self, clus_use, type="all", bin_size=0.001, window_size=0.05, ax=None):

        """Plot CCG for clusters in clus_use (list, max length = 2). Supply only one cluster in clus_use for ACG only.
        type: 'all' or 'ccg_only'.
        ax (optional): if supplied len(ax) must be 1 for type='ccg_only' or nclus^2 for type 'all'"""

        def ccg_spike_assemble(clus_use):
            """Assemble an array of sorted spike times and cluIDs for the input cluster ids the list clus_use """
            spikes_all, clus_all = [], []
            [
                (
                    spikes_all.append(self.times[idc]),
                    clus_all.append(np.ones_like(self.times[idc]) * idc),
                )
                for idc in clus_use
            ]
            spikes_all, clus_all = np.concatenate(spikes_all), np.concatenate(clus_all)
            spikes_sorted, clus_sorted = (
                spikes_all[spikes_all.argsort()],
                clus_all[spikes_all.argsort()],
            )

            return spikes_sorted, clus_sorted.astype("int")

        spikes_sorted, clus_sorted = ccg_spike_assemble(clus_use)
        ccgs = correlograms(
            spikes_sorted,
            clus_sorted,
            sample_rate=self._obj.sampfreq,
            bin_size=bin_size,
            window_size=window_size,
        )

        if type == "ccgs_only":
            ccgs = ccgs[0, 1, :].reshape(1, 1, -1)

        if ax is None:
            fig, ax = plt.subplots(ccgs.shape[0], ccgs.shape[1])

        winsize_bins = 2 * int(0.5 * window_size / bin_size) + 1
        bins = np.linspace(0, 1, winsize_bins)
        for a, ccg in zip(ax.reshape(-1), ccgs.reshape(-1, ccgs.shape[2])):
            a.bar(bins, ccg, width=1 / (winsize_bins - 1))
            a.set_xticks([0, 1])
            a.set_xticklabels(np.ones((2,)) * np.round(window_size / 2, 2))
            a.set_xlabel("Time (s)")
            a.set_ylabel("Spike Count")
            pretty_plot(a)

        return ax

    def roughmua2neuroscope(self, chans, shankIDs):
        """Exports all threshold crossings on electrodes entered to view in neuroscope to .clu.shankID and .res.shankID
        files for viewing in neuroscope.  Must run Spikes.load_mua first. Cluster ids correspond to electrodes.
        Chans = channel numbers in neuroscope."""
        assert len(chans) == len(
            shankIDs
        ), "electrodes and shankIDs must be the same length"

        # re-order/name the channels to match that in the .mua.hdf5 file
        chans_reorder = np.asarray(
            [np.where(chan == self._obj.goodchans)[0][0] for chan in chans]
        )
        for shank in np.unique(shankIDs):

            # Build up array of mua threshold crossings
            spikes, chanIDs = [], []
            for mua_chan, chan in zip(
                chans_reorder[shankIDs == shank], np.asarray(chans)[shankIDs == shank]
            ):

                spikes.extend(self.rough_mua["spiketimes"]["elec_" + str(mua_chan)][:])
                chanIDs.extend(
                    chan
                    * np.ones_like(
                        self.rough_mua["spiketimes"]["elec_" + str(mua_chan)[:]]
                    )
                )

            # sort by spike-times
            sort_ids = np.asarray(spikes).argsort()
            spikes_sorted = np.asarray(spikes)[sort_ids]
            nclu = len(chans)
            chanIDs_sorted = np.append(nclu, np.asarray(chanIDs)[sort_ids])

            # Now write to clu and res files
            mua_filePrefix = self._obj.basePath / (
                self._obj.files.filePrefix.name + "_mua"
            )
            file_clu = mua_filePrefix.with_suffix(".clu." + str(shank))
            file_res = mua_filePrefix.with_suffix(".res." + str(shank))
            with file_clu.open("w") as f_clu, file_res.open("w") as f_res:
                for item in chanIDs_sorted:
                    f_clu.write(f"{item}\n")
                for frame in spikes_sorted:
                    f_res.write(f"{frame}\n")
