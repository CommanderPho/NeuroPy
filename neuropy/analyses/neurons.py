import os
from dataclasses import dataclass
from pathlib import Path

import h5py
import matplotlib as mpl
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.signal as sg
from scipy.ndimage import gaussian_filter
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler


from ..utils.ccg import correlograms
from ..parsePath import Recinfo
from ..plotUtil import pretty_plot
from ..core import Neurons


class Spikes:
    """Spike related methods

    Attributes
    ----------
    times : list
        list of arrays representing spike times of all detected clusters
    pyr : list
        Each array within list representing spike times of pyramidal neurons
    intneur : list
        list of arrays representing spike times of interneurons neurons
    mua : list
        list of arrays representing spike times of mulitunits neurons


    Methods
    ----------
    gen_instfiring()
        generates instantenous firing rate and includes all spiking events (pyr, mua, intneur)

    """

    colors = {"pyr": "#211c1c", "intneur": "#3a924d", "mua": "#b4b2b1"}

    def __init__(self, basepath):

        if isinstance(basepath, Recinfo):
            self._obj = basepath
        else:
            self._obj = Recinfo(basepath)

        self.stability = Stability(basepath)
        # self.dynamics = firingDynamics(basepath)
        filePrefix = self._obj.files.filePrefix

        @dataclass
        class files:
            spikes: str = Path(str(filePrefix) + "_spikes.npy")
            instfiring: str = Path(str(filePrefix) + "_instfiring.pkl")

        self.files = files()

        filename = self.files.spikes
        if filename.is_file():
            self.load_spikes(filename)

    def load_rough_mua(self, mua_filename=None):
        """Load in multi-unit activity (MUA) generated by running spyking-circus with the 'thresholding' flag.
        Note that this differs from MUA designated in phy after manual spike sorting (q = 6),
        so it is defined as Spikes.rough_mua"""

        # First grab the mua file if not specified.
        if mua_filename is None:
            mua_filename = list(self._obj.basePath.glob("**/*.mua.hdf5"))
            assert (
                len(mua_filename) == 1
            ), "More than one .mua.hdf5 file found in directory tree. Re-organize and try again!"

        # Now load it in
        self.rough_mua = h5py.File(mua_filename[0], "r+")

    def get_spkcounts(self, cell_ids, period, binsize=0.25):
        """Get binned spike counts within a period for the given cells"""
        bins = np.arange(period[0], period[1] + binsize, binsize)
        return np.asarray([np.histogram(self.times[_], bins=bins)[0] for _ in cell_ids])

    @property
    def instfiring(self):
        if self.files.instfiring.is_file():
            return pd.read_pickle(self.files.instfiring)
        else:
            print("instantenous file does not exist ")

    def gen_instfiring(self):
        spkall = np.concatenate(self.times)

        bins = np.arange(spkall.min(), spkall.max(), 0.001)

        spkcnt = np.histogram(spkall, bins=bins)[0]
        gaussKernel = self._gaussian()
        instfiring = sg.convolve(spkcnt, gaussKernel, mode="same", method="direct")

        data = pd.DataFrame({"time": bins[1:], "frate": instfiring})
        data.to_pickle(self.files.instfiring)
        return data

    def _gaussian(self):
        """Gaussian function for generating instantenous firing rate

        Returns:
            [array] -- [gaussian kernel centered at zero and spans from -1 to 1 seconds]
        """

        sigma = 0.020
        binSize = 0.001
        t_gauss = np.arange(-1, 1, binSize)
        A = 1 / np.sqrt(2 * np.pi * sigma ** 2)
        gaussian = A * np.exp(-(t_gauss ** 2) / (2 * sigma ** 2))

        return gaussian

    def plot_raster(
        self,
        spikes=None,
        ax=None,
        period=None,
        sort_by_frate=False,
        tstart=0,
        color=None,
        marker="|",
        markersize=2,
        add_vert_jitter=False,
    ):
        """creates raster plot using spike times

        Parameters
        ----------
        spikes : list, optional
            Each array within list represents spike times of that unit, by default None
        ax : obj, optional
            axis to plot onto, by default None
        period : array like, optional
            only plot raster for spikes within this period, by default None
        sort_by_frate : bool, optional
            If true then sorts spikes by the number of spikes (frate), by default False
        tstart : int, optional
            positions the x-axis labels to start from this, by default 0
        color : [type], optional
            color for raster plots, by default None
        marker : str, optional
            marker style, by default "|"
        markersize : int, optional
            size of marker, by default 2
        add_vert_jitter: boolean, optional
            adds vertical jitter to help visualize super dense spiking, not standardly used for rasters...
        """
        if ax is None:
            fig = plt.figure(1, figsize=(6, 10))
            gs = gridspec.GridSpec(1, 1, figure=fig)
            fig.subplots_adjust(hspace=0.4)
            ax = fig.add_subplot(gs[0])

        if spikes is None:
            pyr = self.pyr
            intneur = self.intneur
            mua = self.mua
            spikes = mua + pyr + intneur

            color = (
                [self.colors["mua"]] * len(mua)
                + [self.colors["pyr"]] * len(pyr)
                + [self.colors["intneur"]] * len(intneur)
            )

            # --- mimics legend for labeling unit category ---------
            y = 0.5
            for cell in self.colors:
                ax.annotate(
                    cell,
                    xy=(0.9, y),
                    xycoords="figure fraction",
                    color=self.colors[cell],
                )
                y -= 0.05
        else:
            assert isinstance(spikes, list), "Please provide a list of arrays"
            if color is None:
                color = ["#2d3143"] * len(spikes)
            elif isinstance(color, str):

                try:
                    cmap = mpl.cm.get_cmap(color)
                    color = [cmap(_ / len(spikes)) for _ in range(len(spikes))]
                except:
                    color = [color] * len(spikes)

        # print(f"Plotting {len(spikes)} cells")
        frate = [len(cell) for cell in spikes]  # number of spikes ~= frate

        if period is not None:
            period_duration = np.diff(period)
            spikes = [
                cell[np.where((cell > period[0]) & (cell < period[1]))[0]]
                for cell in spikes
            ]
            frate = np.asarray(
                [len(cell) / period_duration for cell in spikes]
            ).squeeze()

        if sort_by_frate:
            sort_frate_indices = np.argsort(frate)
            spikes = [spikes[indx] for indx in sort_frate_indices]

        for cell, spk in enumerate(spikes):
            if add_vert_jitter:
                jitter_add = np.random.randn(len(spk)) * 0.1
                alpha_use = 0.25
            else:
                jitter_add, alpha_use = 0, 0.5
            ax.plot(
                spk - tstart,
                (cell + 1) * np.ones(len(spk)) + jitter_add,
                marker,
                markersize=markersize,
                color=color[cell],
                alpha=alpha_use,
            )

        ax.set_ylim([0.5, len(spikes) + 0.5])
        ax.set_xlabel("Time (s)")
        ax.set_ylabel("Units")

        return ax

    def plot_ccg(self, clus_use, type="all", bin_size=0.001, window_size=0.05, ax=None):

        """Plot CCG for clusters in clus_use (list, max length = 2). Supply only one cluster in clus_use for ACG only.
        type: 'all' or 'ccg_only'.
        ax (optional): if supplied len(ax) must be 1 for type='ccg_only' or nclus^2 for type 'all'"""

        def ccg_spike_assemble(clus_use):
            """Assemble an array of sorted spike times and cluIDs for the input cluster ids the list clus_use """
            spikes_all, clus_all = [], []
            [
                (
                    spikes_all.append(self.times[idc]),
                    clus_all.append(np.ones_like(self.times[idc]) * idc),
                )
                for idc in clus_use
            ]
            spikes_all, clus_all = np.concatenate(spikes_all), np.concatenate(clus_all)
            spikes_sorted, clus_sorted = (
                spikes_all[spikes_all.argsort()],
                clus_all[spikes_all.argsort()],
            )

            return spikes_sorted, clus_sorted.astype("int")

        spikes_sorted, clus_sorted = ccg_spike_assemble(clus_use)
        ccgs = correlograms(
            spikes_sorted,
            clus_sorted,
            sample_rate=self._obj.sampfreq,
            bin_size=bin_size,
            window_size=window_size,
        )

        if type == "ccgs_only":
            ccgs = ccgs[0, 1, :].reshape(1, 1, -1)

        if ax is None:
            fig, ax = plt.subplots(ccgs.shape[0], ccgs.shape[1])

        winsize_bins = 2 * int(0.5 * window_size / bin_size) + 1
        bins = np.linspace(0, 1, winsize_bins)
        for a, ccg in zip(ax.reshape(-1), ccgs.reshape(-1, ccgs.shape[2])):
            a.bar(bins, ccg, width=1 / (winsize_bins - 1))
            a.set_xticks([0, 1])
            a.set_xticklabels(np.ones((2,)) * np.round(window_size / 2, 2))
            a.set_xlabel("Time (s)")
            a.set_ylabel("Spike Count")
            pretty_plot(a)

        return ax

    def roughmua2neuroscope(self, chans, shankIDs):
        """Exports all threshold crossings on electrodes entered to view in neuroscope to .clu.shankID and .res.shankID
        files for viewing in neuroscope.  Must run Spikes.load_mua first. Cluster ids correspond to electrodes.
        Chans = channel numbers in neuroscope."""
        assert len(chans) == len(
            shankIDs
        ), "electrodes and shankIDs must be the same length"

        # re-order/name the channels to match that in the .mua.hdf5 file
        chans_reorder = np.asarray(
            [np.where(chan == self._obj.goodchans)[0][0] for chan in chans]
        )
        for shank in np.unique(shankIDs):

            # Build up array of mua threshold crossings
            spikes, chanIDs = [], []
            for mua_chan, chan in zip(
                chans_reorder[shankIDs == shank], np.asarray(chans)[shankIDs == shank]
            ):

                spikes.extend(self.rough_mua["spiketimes"]["elec_" + str(mua_chan)][:])
                chanIDs.extend(
                    chan
                    * np.ones_like(
                        self.rough_mua["spiketimes"]["elec_" + str(mua_chan)[:]]
                    )
                )

            # sort by spike-times
            sort_ids = np.asarray(spikes).argsort()
            spikes_sorted = np.asarray(spikes)[sort_ids]
            nclu = len(chans)
            chanIDs_sorted = np.append(nclu, np.asarray(chanIDs)[sort_ids])

            # Now write to clu and res files
            mua_filePrefix = self._obj.basePath / (
                self._obj.files.filePrefix.name + "_mua"
            )
            file_clu = mua_filePrefix.with_suffix(".clu." + str(shank))
            file_res = mua_filePrefix.with_suffix(".res." + str(shank))
            with file_clu.open("w") as f_clu, file_res.open("w") as f_res:
                for item in chanIDs_sorted:
                    f_clu.write(f"{item}\n")
                for frame in spikes_sorted:
                    f_res.write(f"{frame}\n")
